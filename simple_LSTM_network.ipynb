{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple LSTM\n",
    "\n",
    "so basic it runs on pumpkin spice\n",
    "\n",
    "## echo sequence prediction problem\n",
    "\n",
    "### generating data\n",
    "\n",
    "our echo sequence prediction problem needs data: specifically vectors of random sequences. let's use integers, and define our problem space as integers between 0 and 99.\n",
    "\n",
    "we'll use the ```randint()``` function from the python 3 ```random``` [module](https://docs.python.org/3/library/random.html \"python 3 random module docs\") to generate random integers within the range we specify (in this case, 0 to 99). \n",
    "\n",
    "we can use the ```randint()``` function within a function of our own to generate sequences of random integers--this will be the data for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randint() is inside the python random module\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use randint() to generate a random integer between 0 and 99\n",
    "\n",
    "rand_int = random.randint(0, 99)\n",
    "\n",
    "rand_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need a _lot_ more than one of these. which means it's time to build a function to automate this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq(seq_length, n_features):\n",
    "    \n",
    "    '''\n",
    "    generate sequences of a given length\n",
    "    and given number of features\n",
    "    '''\n",
    "    return [random.randint(0, n_features - 1) for _ in range(seq_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__demo:__ let's make a sequence with 10 values and 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44, 21, 1, 7, 16, 8, 49, 48, 38, 20]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_seq(10, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding\n",
    "\n",
    "before we can train the model, we have to encode the data into a format that an LSTM can use. the way we encode data matters; choices made here can significantly affect model performance.\n",
    "\n",
    "to frame this data properly, let's revisit the original problem:\n",
    "\n",
    "we're trying to predict a number. a _specific_ number.\n",
    "\n",
    "if we wanted to _approximate_ the number, we could frame this as a __regression__ problem, and train our model to output a close (but not exact) approximation of the number.\n",
    "\n",
    "but because we want the _exact_ integer (and _not_ an approximation, which is what a regression model outputs) we need to frame this problem as a __classification__ model.\n",
    "\n",
    "__classification__ means handling categorical data, which machines can do handily using __one hot encoding__.\n",
    "\n",
    "### automatic vs manual one hot encoding\n",
    "\n",
    "```scikit-learn``` has a super neat ```OneHotEncoder()``` [transformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \"sklearn OneHotEncoder doc\") that can automate one hot encoding, but because it fits the data, it can only encode the values that it sees represented. \n",
    "\n",
    "we need all possible values--from 0 to 99--represented. but because we're generating our integer sequences pseudo-randomly using ```np.random.randint()```, we can't guarantee that all values will be represented.\n",
    "\n",
    "it's possible to feed in the categories to ```OneHotEncoder()``` manually. here, however, we're going to simply make our own transformer.\n",
    "\n",
    "we'll convert the results to a ```numpy``` ```array``` in order to make them easier to decode later.\n",
    "\n",
    "### decoding\n",
    "\n",
    "later on we'll need a way to interpret the model's results. to do so we'll need to decode the one hot scheme.\n",
    "\n",
    "we can easily do this using the ```numpy``` ```argmax()``` function.\n",
    "\n",
    "```numpy.argmax()``` returns the indices for the maximum values along a vector. because each vector in the binary one hot encoding will be a lot of zeroes with a single high value--a ```1```--we can easily use ```argmax()``` to grab the index of the ```1``` value and return it. that's our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "# encoder function\n",
    "\n",
    "def one_hot_encoder(seq, n_features):\n",
    "    \n",
    "    '''\n",
    "    creates a vector of binary values for each\n",
    "    possible feature in the dataset.\n",
    "    '''\n",
    "    \n",
    "    encoding = list()\n",
    "    \n",
    "    for val in seq:\n",
    "        \n",
    "        vector = [0 for _ in range(n_features)]\n",
    "        vector[val] = 1\n",
    "        encoding.append(vector)\n",
    "        \n",
    "    return array(encoding)  \n",
    "\n",
    "# decoder function\n",
    "\n",
    "def one_hot_decoder(seq_encoded):\n",
    "    '''\n",
    "    decodes results by returning the index of\n",
    "    the point in the vector with the largest value,\n",
    "    i.e. 1 \n",
    "    '''\n",
    "    \n",
    "    return [argmax(vector) for vector in seq_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 40, 23, 75, 33, 81, 25, 0, 79, 63, 10, 66, 39, 95, 74, 23, 62, 61, 35, 58, 56, 84, 91, 15, 4, 31, 99, 47, 39, 61, 58, 97, 45, 61, 41, 78, 48, 87, 27, 60, 27, 14, 39, 79, 32, 35, 10, 75, 18, 81] \n",
      "\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] \n",
      "\n",
      "[12, 3, 22, 31, 50, 85, 45, 94, 56, 15, 97, 87, 10, 86, 74, 22, 24, 20, 27, 39, 89, 56, 19, 44, 19, 9, 50, 64, 0, 67, 94, 22, 4, 7, 52, 10, 49, 72, 33, 43, 33, 47, 24, 85, 28, 72, 67, 66, 91, 49]\n"
     ]
    }
   ],
   "source": [
    "sequence = make_seq(50, 100)\n",
    "\n",
    "seq_encoded = one_hot_encoder(sequence, 100)\n",
    "\n",
    "seq_decoded = one_hot_decoder(seq_encoded)\n",
    "\n",
    "print(seq, '\\n')\n",
    "print(seq_encoded, '\\n')\n",
    "print(seq_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape to 3d matrix\n",
    "\n",
    "LSTMs require input in the form of a 3d matrix.\n",
    "\n",
    "the three dimensions LSTMs need, in order, are: __samples, time steps, & features__.\n",
    "\n",
    "for the specific sequence we just generated, it's easy to set the shape to three dimensions using the ```reshape()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]]\n",
      "(1, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "X = seq_encoded.reshape(1, 50, 100)\n",
    "\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a more generalizable version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoded.reshape(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more information\n",
    "\n",
    "##### python 3 random module documentation:\n",
    "\n",
    "https://docs.python.org/3/library/random.html\n",
    "\n",
    "##### sklearn preprocessing documentation:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
