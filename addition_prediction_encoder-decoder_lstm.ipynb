{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addition prediction problem & encoder-decoder lstm\n",
    "\n",
    "in their 2015 [paper](https://arxiv.org/pdf/1410.4615.pdf \"https://arxiv.org/pdf/1410.4615.pdf\"), Wojciech Zaremba and Ilya Sutskever showed that LSTM encoder-decoder models were capable of calculating the output of small programs--adding together two numbers of up to nine digits in length.\n",
    "\n",
    "even more impressive, the model was reading the character representations of the symbols and digits. there is nothing programatically to indicate to the model what operations are being represented--it learns this during training:\n",
    "\n",
    ">*It is important to emphasize that the LSTM reads the entire input one character at a time and produces the output one character at a time. The characters are initially meaningless from the model’s\n",
    "perspective;  for instance, the model does not know that “+” means addition or that\n",
    "6\n",
    "is followed\n",
    "by\n",
    "7\n",
    ".  In fact, scrambling the input characters (e.g., replacing “a” with “q”, “b” with “w”, etc.,) has\n",
    "no effect on the model’s ability to solve this problem*\n",
    ">\n",
    "> Wojciech Zaremba and Ilya Sutskever, Learning to Execute\n",
    "\n",
    "as an example of how this works, the model might take in the sequence representing\n",
    "\n",
    "12 + 4 = 16\n",
    "\n",
    "represented in the following vectors:\n",
    "\n",
    "`['1','2','+','5']`\n",
    "\n",
    "`['1','7']`\n",
    "\n",
    "the digits and symbols are just characters; they have no fucntional meaning. the model learns the relationships during training.\n",
    "\n",
    "\n",
    "## sequence-to-sequence (seq2seq) model\n",
    "\n",
    "there are a couple of key characteristics to note in this problem. first, the order matters--shuffling the order of the characters would make any relationship impossible to deduce.\n",
    "\n",
    "second, the input and output can vary, making this problem more challenging than a one-to-one or many-to-one sequence prediction problem.\n",
    "\n",
    "because the data is an ordered sequence of variable input and output length, this problem requires a many-to-many modeling approach, otherwise known as [__sequence to sequence__](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf \"https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\").\n",
    "\n",
    "### padding\n",
    "\n",
    "because sequence lengths can vary, we need to do some __padding__. padding consists of adding characters at the beginning, end (or both) of a sequence to make sure all the sequences in a training set are the same length. any character can be used; but it should be sufficiently different from the training data that the mahcine can figure out it isn't pertinent to the problem at hand--in other words, the padding shoulnd't add noise.\n",
    "\n",
    "there are different methods for choosing what characters to use for padding and how. some are intuitive, and some are statistical. \n",
    "\n",
    "*for more information about libraries for padding:*\n",
    "\n",
    "__tensorflow__\n",
    "\n",
    "*tool:* `tf.pad`\n",
    "\n",
    "*documentation:* https://www.tensorflow.org/api_docs/python/tf/pad\n",
    "\n",
    "__keras__\n",
    "\n",
    "*tool:* `pad_sequences`\n",
    "\n",
    "*documentation:* https://keras.io/preprocessing/sequence/\n",
    "\n",
    "__numpy__\n",
    "\n",
    "*tool:* `numpy.pad`\n",
    "\n",
    "*documentation:* https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.pad.html\n",
    "\n",
    "### one hot encoding\n",
    "\n",
    "in order to be machine readable, these characters need to somehow be encoded into numerical data.\n",
    "\n",
    "in this case, [__one hot encoding__](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f \"https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\") provides the answer.\n",
    "\n",
    "we can treat each character as a category, and each input to the machine will come with a set of vectors defining it. it's interesting to note here that in a sense the dense matrices associated with one hot encoding mostly serve to tell a machine what categories a particular example *isn't* in; these matrices are mostly zeros.\n",
    "\n",
    "for automatic one hot encoding, there are a number of libraries and packages available. these mostly work well when every potential category is represented in the data.\n",
    "\n",
    "\n",
    "### generating data\n",
    "\n",
    "the code below generates data perfectly prepared for this problem. when executed, it will\n",
    "\n",
    "* __generate__ random pairs of numbers with their sums\n",
    "* convert the __integers__ to __strings__\n",
    "* __pad__ the strings on the left, using the space `' '` character\n",
    "* __integer encode__ the sequences\n",
    "\n",
    "and, finally\n",
    "\n",
    "* __one hot encode__ the sequences\n",
    "* __assign__ sequences to data structures (lists) so they're ready for a model\n",
    "\n",
    "because this code does all these things manually, it's easy to see how many of these parameters can be changed to experiement with model performance!\n",
    "\n",
    "the code to generate this data below is taken (with only a few modifications) from [jason brownlee's excellent course on LSTMs](https://machinelearningmastery.com/lstms-with-python/ \"https://machinelearningmastery.com/lstms-with-python/\") available at [www.machinelearningmastery.com](https://machinelearningmastery.com/lstms-with-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: pairs with sums \n",
      "\n",
      "[[3, 10]] [13]\n",
      "\n",
      " step 2: transform to strings \n",
      "\n",
      "['3 + 10'] ['13']\n",
      "\n",
      " step 3: integer encoding \n",
      "\n",
      "[[3, 11, 10, 11, 1, 0]] [[1, 3]]\n",
      "\n",
      " final step: one hot encoding \n",
      "\n",
      "[[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]] [[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randint\n",
    "from math import ceil\n",
    "from math import log10\n",
    "\n",
    "# generate lists of random integers and their sum\n",
    "\n",
    "def random_sum_pairs(n_examples, n_numbers, largest):\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for _ in range(n_examples):\n",
    "        \n",
    "        in_pattern = [randint(1,largest) for _ in range(n_numbers)]\n",
    "        \n",
    "        out_pattern = sum(in_pattern)\n",
    "        \n",
    "        X.append(in_pattern)\n",
    "        \n",
    "        y.append(out_pattern)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# convert data to strings\n",
    "\n",
    "def to_string(X, y, n_numbers, largest):\n",
    "    \n",
    "    # calculate largest possible value\n",
    "    \n",
    "    max_length = int(n_numbers * ceil(log10(largest+1)) + n_numbers - 1)\n",
    "    \n",
    "    Xstr = list()\n",
    "    \n",
    "    for pattern in X:\n",
    "        \n",
    "        strp = ' + ' .join([str(n) for n in pattern])\n",
    "        \n",
    "        strp = '' .join([ ' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        \n",
    "        Xstr.append(strp)\n",
    "    \n",
    "    max_length = int(ceil(log10(n_numbers * (largest+1))))\n",
    "    \n",
    "    ystr = list()\n",
    "    \n",
    "    for pattern in y:\n",
    "        \n",
    "        strp = str(pattern)\n",
    "        \n",
    "        strp = '' .join([ ' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        \n",
    "        ystr.append(strp)\n",
    "    \n",
    "    return Xstr, ystr\n",
    "\n",
    "# integer encode strings\n",
    "# i've changed variable names here to prevent scope issues with multiple notebook runs\n",
    "# and make it easier for me to read\n",
    "\n",
    "def integer_encode(X, y, alphabet):\n",
    "    \n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    X_int = list()\n",
    "    \n",
    "    for pattern in X:\n",
    "        \n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        \n",
    "        X_int.append(integer_encoded)\n",
    "    \n",
    "    y_int = list()\n",
    "    \n",
    "    for pattern in y:\n",
    "        \n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        \n",
    "        y_int.append(integer_encoded)\n",
    "    \n",
    "    return X_int, y_int\n",
    "\n",
    "# one hot encode\n",
    "# some names changed for scope & readability\n",
    "\n",
    "def one_hot_encode(X, y, max_int):\n",
    "    \n",
    "    X_encoded = list()\n",
    "    \n",
    "    for seq in X:\n",
    "        \n",
    "        pattern = list()\n",
    "            \n",
    "        for index in seq:\n",
    "                \n",
    "            vector = [0 for _ in range(max_int)]\n",
    "                \n",
    "            vector[index] = 1\n",
    "                \n",
    "            pattern.append(vector)\n",
    "        \n",
    "        X_encoded.append(pattern)\n",
    "    \n",
    "    y_encoded = list()\n",
    "    \n",
    "    for seq in y:\n",
    "        \n",
    "        pattern = list()\n",
    "        \n",
    "        for index in seq:\n",
    "            \n",
    "            vector = [0 for _ in range(max_int)]\n",
    "            \n",
    "            vector[index] = 1\n",
    "            \n",
    "            pattern.append(vector)\n",
    "        \n",
    "        y_encoded.append(pattern)\n",
    "    \n",
    "    return X_encoded, y_encoded\n",
    "\n",
    "# let's test it out\n",
    "# to make it easy to see how pieces fit i've numbered the X & y transforms\n",
    "# X_1, X_2, y_3, etc...to X_final, y_final\n",
    "\n",
    "seed(1)\n",
    "\n",
    "n_samples = 1\n",
    "\n",
    "n_numbers = 2\n",
    "\n",
    "largest_number = 10\n",
    "\n",
    "# make pairs\n",
    "\n",
    "X_1, y_1 = random_sum_pairs(n_samples, n_numbers, largest_number)\n",
    "\n",
    "print('step 1: pairs with sums \\n')\n",
    "print(X_1, y_1)\n",
    "\n",
    "# convert to strings\n",
    "\n",
    "X_2, y_2 = to_string(X_1, y_1, n_numbers, largest_number)\n",
    "\n",
    "print('\\n step 2: transform to strings \\n')\n",
    "print(X_2, y_2)\n",
    "\n",
    "# integer encode\n",
    "# include every character we're using\n",
    "# even the spaces for padding!\n",
    "\n",
    "alphabet = ['0','1','2','3','4','5','6','7','8','9','+',' ']\n",
    "\n",
    "X_3, y_3 = integer_encode(X_2, y_2, alphabet)\n",
    "\n",
    "print('\\n step 3: integer encoding \\n')\n",
    "print(X_3, y_3)\n",
    "\n",
    "# one hot encode\n",
    "\n",
    "X_final, y_final = one_hot_encode(X_3, y_3, len(alphabet))\n",
    "\n",
    "print('\\n final step: one hot encoding \\n')\n",
    "print(X_final, y_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sweet, it works! obvs we're not going to want to do all that for every sample, so let's make a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(n_samples, n_numbers, largest_number, alphabet):\n",
    "    \n",
    "    # get pairs\n",
    "    \n",
    "    X_pairs, y_pairs = random_sum_pairs(n_samples, n_numbers, largest_number)\n",
    "    \n",
    "    # pairs to strings\n",
    "    \n",
    "    X_strings, y_strings = to_strings(X_pairs, y_pairs, largest_number)\n",
    "    \n",
    "    # integer encode\n",
    "    \n",
    "    X_int, y_int = integer_encode(X_strings, y_strings, alphabet)\n",
    "    \n",
    "    # one hot encode\n",
    "    \n",
    "    X_encoded, y_encoded = one_hot_encode(X_int, y_int, len(alphabet))\n",
    "    \n",
    "    # return as numpy arrays\n",
    "    \n",
    "    X, y = array(X_encoded), array(y_encoded)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding sequences\n",
    "\n",
    "in order to easily read the results, we'll need to decode them. with one hot encoding, it's easy to use python's `argmax` to get the results: in a matrix of (nearly) all zeros, the highest value will be the `1` denoting a character's category. \n",
    "\n",
    "the results can be inverted using `argmax()` to return the index of the category a particular character belongs to, since the `1` denoting it will be the highest number in the otherwise all-zero array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_results(sequence, alphabet):\n",
    "    \n",
    "    int_2_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    strings = list()\n",
    "    \n",
    "    for seq in sequences:\n",
    "        \n",
    "        string_version = int_2_char[argmax(seq)]\n",
    "        \n",
    "        strings.append(string_version)\n",
    "    \n",
    "    return ''.join(strings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
