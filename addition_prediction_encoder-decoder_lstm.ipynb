{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addition prediction problem & encoder-decoder lstm\n",
    "\n",
    "in their 2015 [paper](https://arxiv.org/pdf/1410.4615.pdf \"https://arxiv.org/pdf/1410.4615.pdf\"), Wojciech Zaremba and Ilya Sutskever showed that LSTM encoder-decoder models were capable of calculating the output of small programs--adding together two numbers of up to nine digits in length.\n",
    "\n",
    "even more impressive, the model was reading the character representations of the symbols and digits. there is nothing programatically to indicate to the model what operations are being represented--it learns this during training:\n",
    "\n",
    ">*It is important to emphasize that the LSTM reads the entire input one character at a time and produces the output one character at a time. The characters are initially meaningless from the model’s\n",
    "perspective;  for instance, the model does not know that “+” means addition or that\n",
    "6\n",
    "is followed\n",
    "by\n",
    "7\n",
    ".  In fact, scrambling the input characters (e.g., replacing “a” with “q”, “b” with “w”, etc.,) has\n",
    "no effect on the model’s ability to solve this problem*\n",
    ">\n",
    "> Wojciech Zaremba and Ilya Sutskever, Learning to Execute\n",
    "\n",
    "as an example of how this works, the model might take in the sequence representing\n",
    "\n",
    "12 + 4 = 16\n",
    "\n",
    "represented in the following vectors:\n",
    "\n",
    "`['1','2','+','5']`\n",
    "\n",
    "`['1','7']`\n",
    "\n",
    "the digits and symbols are just characters; they have no fucntional meaning. the model learns the relationships during training.\n",
    "\n",
    "\n",
    "## sequence-to-sequence (seq2seq) model\n",
    "\n",
    "there are a couple of key characteristics to note in this problem. first, the order matters--shuffling the order of the characters would make any relationship impossible to deduce.\n",
    "\n",
    "second, the input and output can vary, making this problem more challenging than a one-to-one or many-to-one sequence prediction problem.\n",
    "\n",
    "because the data is an ordered sequence of variable input and output length, this problem requires a many-to-many modeling approach, otherwise known as [__sequence to sequence__](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf \"https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\").\n",
    "\n",
    "### padding\n",
    "\n",
    "because sequence lengths can vary, we need to do some __padding__. padding consists of adding characters at the beginning, end (or both) of a sequence to make sure all the sequences in a training set are the same length. any character can be used; but it should be sufficiently different from the training data that the mahcine can figure out it isn't pertinent to the problem at hand--in other words, the padding shoulnd't add noise.\n",
    "\n",
    "there are different methods for choosing what characters to use for padding and how. some are intuitive, and some are statistical. \n",
    "\n",
    "*for more information about libraries for padding:*\n",
    "\n",
    "__tensorflow__\n",
    "\n",
    "*tool:* `tf.pad`\n",
    "\n",
    "*documentation:* https://www.tensorflow.org/api_docs/python/tf/pad\n",
    "\n",
    "__keras__\n",
    "\n",
    "*tool:* `pad_sequences`\n",
    "\n",
    "*documentation:* https://keras.io/preprocessing/sequence/\n",
    "\n",
    "__numpy__\n",
    "\n",
    "*tool:* `numpy.pad`\n",
    "\n",
    "*documentation:* https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.pad.html\n",
    "\n",
    "### one hot encoding\n",
    "\n",
    "in order to be machine readable, these characters need to somehow be encoded into numerical data.\n",
    "\n",
    "in this case, [__one hot encoding__](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f \"https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\") provides the answer.\n",
    "\n",
    "we can treat each character as a category, and each input to the machine will come with a set of vectors defining it. it's interesting to note here that in a sense the dense matrices associated with one hot encoding mostly serve to tell a machine what categories a particular example *isn't* in; these matrices are mostly zeros.\n",
    "\n",
    "for automatic one hot encoding, there are a number of libraries and packages available. these mostly work well when every potential category is represented in the data.\n",
    "\n",
    "\n",
    "### generating data\n",
    "\n",
    "the code below generates data perfectly prepared for this problem. when executed, it will\n",
    "\n",
    "* __generate__ random pairs of numbers with their sums\n",
    "* convert the __integers__ to __strings__\n",
    "* __pad__ the strings on the left, using the space `' '` character\n",
    "* __integer encode__ the sequences\n",
    "\n",
    "and, finally\n",
    "\n",
    "* __one hot encode__ the sequences\n",
    "* __assign__ sequences to data structures (lists) so they're ready for a model\n",
    "\n",
    "because this code does all these things manually, it's easy to see how many of these parameters can be changed to experiement with model performance!\n",
    "\n",
    "the code to generate this data below is taken (with only a few modifications) from [jason brownlee's excellent course on LSTMs](https://machinelearningmastery.com/lstms-with-python/ \"https://machinelearningmastery.com/lstms-with-python/\") available at [www.machinelearningmastery.com](https://machinelearningmastery.com/lstms-with-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: pairs with sums \n",
      "\n",
      "[[3, 10, 2]] [15]\n",
      "\n",
      " step 2: transform to strings \n",
      "\n",
      "['  3+10+2'] ['15']\n",
      "\n",
      " step 3: integer encoding \n",
      "\n",
      "[[11, 11, 3, 10, 1, 0, 10, 2]] [[1, 5]]\n",
      "\n",
      " final step: one hot encoding \n",
      "\n",
      "[[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]] \n",
      "\n",
      "[[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]]]\n",
      "\n",
      "\n",
      "(1, 8, 12)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from random import seed\n",
    "from random import randint\n",
    "from math import ceil\n",
    "from math import log10\n",
    "\n",
    "# generate lists of random integers and their sum\n",
    "\n",
    "def random_sum_pairs(n_examples, n_numbers, largest):\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    \n",
    "    for _ in range(n_examples):\n",
    "        \n",
    "        in_pattern = [randint(1,largest) for _ in range(n_numbers)]\n",
    "        \n",
    "        out_pattern = sum(in_pattern)\n",
    "        \n",
    "        X.append(in_pattern)\n",
    "        \n",
    "        y.append(out_pattern)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# convert data to strings\n",
    "\n",
    "def to_string(X, y, n_numbers, largest):\n",
    "    \n",
    "    # calculate largest possible value\n",
    "    \n",
    "    max_length = int(n_numbers * ceil(log10(largest+1)) + n_numbers - 1)\n",
    "    \n",
    "    Xstr = list()\n",
    "    \n",
    "    for pattern in X:\n",
    "        \n",
    "        strp = '+'.join([str(n) for n in pattern])\n",
    "        \n",
    "        strp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        \n",
    "        Xstr.append(strp)\n",
    "    \n",
    "    max_length = int(ceil(log10(n_numbers * (largest+1))))\n",
    "    \n",
    "    ystr = list()\n",
    "    \n",
    "    for pattern in y:\n",
    "        \n",
    "        strp = str(pattern)\n",
    "        \n",
    "        strp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n",
    "        \n",
    "        ystr.append(strp)\n",
    "    \n",
    "    return Xstr, ystr\n",
    "\n",
    "# integer encode strings\n",
    "# i've changed variable names here to prevent scope issues with multiple notebook runs\n",
    "# and make it easier for me to read\n",
    "\n",
    "def integer_encode(X, y, alphabet):\n",
    "    \n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    X_int = list()\n",
    "    \n",
    "    for pattern in X:\n",
    "        \n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        \n",
    "        X_int.append(integer_encoded)\n",
    "    \n",
    "    y_int = list()\n",
    "    \n",
    "    for pattern in y:\n",
    "        \n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        \n",
    "        y_int.append(integer_encoded)\n",
    "    \n",
    "    return X_int, y_int\n",
    "\n",
    "# one hot encode\n",
    "# some names changed for scope & readability\n",
    "\n",
    "def one_hot_encode(X, y, max_int):\n",
    "    \n",
    "    X_encoded = list()\n",
    "    \n",
    "    for seq in X:\n",
    "        \n",
    "        pattern = list()\n",
    "            \n",
    "        for index in seq:\n",
    "                \n",
    "            vector = [0 for _ in range(max_int)]\n",
    "                \n",
    "            vector[index] = 1\n",
    "                \n",
    "            pattern.append(vector)\n",
    "        \n",
    "        X_encoded.append(pattern)\n",
    "    \n",
    "    y_encoded = list()\n",
    "    \n",
    "    for seq in y:\n",
    "        \n",
    "        pattern = list()\n",
    "        \n",
    "        for index in seq:\n",
    "            \n",
    "            vector = [0 for _ in range(max_int)]\n",
    "            \n",
    "            vector[index] = 1\n",
    "            \n",
    "            pattern.append(vector)\n",
    "        \n",
    "        y_encoded.append(pattern)\n",
    "    \n",
    "    return X_encoded, y_encoded\n",
    "\n",
    "# let's test it out\n",
    "# to make it easy to see how pieces fit i've numbered the X & y transforms\n",
    "# X_1, X_2, y_3, etc...to X_final, y_final\n",
    "\n",
    "seed(1)\n",
    "\n",
    "n_samples = 1\n",
    "\n",
    "n_terms = 3\n",
    "\n",
    "largest_number = 10\n",
    "\n",
    "# make pairs\n",
    "\n",
    "X_1, y_1 = random_sum_pairs(n_samples, n_terms, largest_number)\n",
    "\n",
    "print('step 1: pairs with sums \\n')\n",
    "print(X_1, y_1)\n",
    "\n",
    "# convert to strings\n",
    "\n",
    "X_2, y_2 = to_string(X_1, y_1, n_terms, largest_number)\n",
    "\n",
    "print('\\n step 2: transform to strings \\n')\n",
    "print(X_2, y_2)\n",
    "\n",
    "# integer encode\n",
    "# include every character we're using\n",
    "# even the spaces for padding!\n",
    "\n",
    "alphabet = ['0','1','2','3','4','5','6','7','8','9','+',' ']\n",
    "\n",
    "X_3, y_3 = integer_encode(X_2, y_2, alphabet)\n",
    "\n",
    "print('\\n step 3: integer encoding \\n')\n",
    "print(X_3, y_3)\n",
    "\n",
    "# one hot encode\n",
    "\n",
    "X_final, y_final = one_hot_encode(X_3, y_3, len(alphabet))\n",
    "\n",
    "X_array = np.array(X_final)\n",
    "\n",
    "print('\\n final step: one hot encoding \\n')\n",
    "print(X_final, '\\n')\n",
    "print(y_final)\n",
    "print('\\n')\n",
    "print(X_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sweet, it works! obvs we're not going to want to do all that for every sample, so let's make a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def make_samples(n_samples, n_numbers, largest_number, alphabet):\n",
    "    \n",
    "    # get pairs\n",
    "    \n",
    "    X_pairs, y_pairs = random_sum_pairs(n_samples, n_numbers, largest_number)\n",
    "    \n",
    "    # pairs to strings\n",
    "    \n",
    "    X_strings, y_strings = to_string(X_pairs, y_pairs, n_numbers, largest_number)\n",
    "    \n",
    "    # integer encode\n",
    "    \n",
    "    X_int, y_int = integer_encode(X_strings, y_strings, alphabet)\n",
    "    \n",
    "    # one hot encode\n",
    "    \n",
    "    X_encoded, y_encoded = one_hot_encode(X_int, y_int, len(alphabet))\n",
    "    \n",
    "    # return as numpy arrays\n",
    "    \n",
    "    X, y = np.array(X_encoded), np.array(y_encoded)\n",
    "    \n",
    "    return X, y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "  [0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "  [0 0 0 0 0 0 0 0 1 0 0 0]]]\n",
      "(1, 8, 12)\n"
     ]
    }
   ],
   "source": [
    "alphabet = [str(x) for x in range(10)] + ['+', ' ']\n",
    "\n",
    "X_1, y_1 = generate_data(1, 3, 10, alphabet)\n",
    "\n",
    "print(X_1)\n",
    "print(X_1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding sequences\n",
    "\n",
    "in order to easily read the results, we'll need to decode them. with one hot encoding, it's easy to use python's `argmax` to get the results: in a matrix of (nearly) all zeros, the highest value will be the `1` denoting a character's category. \n",
    "\n",
    "the results can be inverted using `argmax()` to return the index of the category a particular character belongs to, since the `1` denoting it will be the highest number in the otherwise all-zero array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_results(sequence, alphabet):\n",
    "    \n",
    "    int_2_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    \n",
    "    strings = list()\n",
    "    \n",
    "    for seq in sequences:\n",
    "        \n",
    "        string_version = int_2_char[argmax(seq)]\n",
    "        \n",
    "        strings.append(string_version)\n",
    "    \n",
    "    return ''.join(strings)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing to compile: setting reusable parameters\n",
    "\n",
    "we'll set a few parameters to the specifics of this problem, then define & compile the model.\n",
    "\n",
    "#### generating data\n",
    "\n",
    "we can generate pairs of integer sets and their sums using the functions above; all we need to do is specify how many digits to add together, and the largest number to include.\n",
    "\n",
    "finally, for encoding we need to give the alphabet--the list of all possible characters the model might encounter.\n",
    "\n",
    "#### problem-specific model parameters\n",
    "\n",
    "we need several variables to maximize code reusability, in case we want to adjust various paramters to tweak the model later.\n",
    "\n",
    "because our __features__ are the possible characters, and because we can choose to use alternate math functions or even padding schemes (thus changing the included characters), we can set this as a variable.\n",
    "\n",
    "the length of an input sequence can be changed--for this problem we will use terms, which if our highest possible number is 10 and we include 2 mathematical operators (like '+' or '-'), is a total of 8 characters--or, in this problem, __time steps__.\n",
    "\n",
    "#### a note on the lstm encoder-decoder architecture\n",
    "\n",
    "the output of an encoder lstm is 2 dimensional--which is a problem since the decoder lstm layer expects a 3 dimensional input. \n",
    "\n",
    "\n",
    "to put the data into the right shape to feed directly into the decoder lstm, we'll use a `RepeatVector` layer. encoded input is repeated once per output time step, adding a 3rd dimension and allowing the lstm encoder layer to connect directly to the lstm decoder layer.\n",
    "\n",
    "the final value to be specified is the number of output time steps to repeat the decoder's output vector--equal to the number of __output time steps__.\n",
    "\n",
    "because the length of the output sequences can vary (since it's a randomly variable addition problem), we need to set this value to the largest possible number of digits in the answer. in this case, the highest possible sum set is 10+10+10 = 30, which has 2 digits or time steps.\n",
    "\n",
    "storing this value in a variable will allow for the parameters of the problem to be easily adjusted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define & compile\n",
    "\n",
    "# set number of terms in addition problem\n",
    "\n",
    "n_terms = 3\n",
    "\n",
    "# largest number in a term\n",
    "\n",
    "largest_number = 10\n",
    "\n",
    "# string versions of all digits, plus symbols used for math & padding\n",
    "# in an iterable list\n",
    "\n",
    "alphabet = [str(x) for x in range(10)] + ['+', ' ']\n",
    "\n",
    "# set number of features\n",
    "# length of alphabet\n",
    "\n",
    "n_char_features = len(alphabet)\n",
    "\n",
    "# length of input sequences\n",
    "# calculate max possible based on problem parameters\n",
    "\n",
    "max_in_seq_length = int(n_terms * ceil(log10(largest_number + 1)) + n_terms -1)\n",
    "\n",
    "# length of output sequences\n",
    "# calculate maximum possible based on problem parameters\n",
    "\n",
    "max_out_seq_length = int(ceil(log10(n_terms * (largest_number + 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test to make sure everything's working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', ' ']\n",
      "\n",
      "\n",
      "12\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(alphabet)\n",
    "print('\\n')\n",
    "print(n_char_features)\n",
    "print('\\n')\n",
    "print(max_in_seq_length)\n",
    "print('\\n')\n",
    "print(max_out_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "  [0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "  [0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "  [0 0 0 0 0 0 0 1 0 0 0 0]]]\n",
      "(1, 8, 12)\n",
      "[[[0 0 1 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0 0 0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "x1, y1 = generate_data(1, n_terms, largest_number, alphabet)\n",
    "\n",
    "print(x1)\n",
    "print(x1.shape)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define & compile\n",
    "\n",
    "the input layer of this model will be an lstm layer with 75 memory cells. its unput shape will be the number of samples, the input sequence length, and the number of possible features (characters).\n",
    "\n",
    "our model uses a `RepeatVector` layer, set to the number of output time steps, to repeat the encoded vector creating a 3d input for the lstm decoder. the decoder will consists of 50 memory cells, and is set to return sequences.\n",
    "\n",
    "finally, a `TimeDistributed` layer wraps a dense output layer, allowing the layer to repeat without changing its weights for as many output time steps as we need. the dense layer uses a softmax activation, and sorts according to the (in this case, 12) possible output classes. \n",
    "\n",
    "the model will be compiled with:\n",
    "\n",
    "* log __loss__\n",
    "\n",
    "* adam gradient descent __optimizer__\n",
    "\n",
    "* accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_37 (LSTM)               (None, 75)                26400     \n",
      "_________________________________________________________________\n",
      "repeat_vector_19 (RepeatVect (None, 2, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 2, 50)             25200     \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 2, 12)             612       \n",
      "=================================================================\n",
      "Total params: 52,212\n",
      "Trainable params: 52,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "# define the model\n",
    "\n",
    "encoder_decoder_model = Sequential()\n",
    "\n",
    "# first lstm layer: the encoder\n",
    "\n",
    "encoder_decoder_model.add(LSTM(75, input_shape=(max_in_seq_length, n_char_features)))\n",
    "\n",
    "#encoder_decoder_model.add(LSTM(75, input_shape=(8, 12)))\n",
    "\n",
    "\n",
    "# RepeatVector adapter layer\n",
    "\n",
    "encoder_decoder_model.add(RepeatVector(max_out_seq_length))\n",
    "\n",
    "# second lstm layer: decoder\n",
    "\n",
    "encoder_decoder_model.add(LSTM(50, return_sequences=True))\n",
    "\n",
    "# TimeDistributed dense layer\n",
    "\n",
    "encoder_decoder_model.add(TimeDistributed(Dense(n_char_features, activation='softmax')))\n",
    "\n",
    "# compile the model\n",
    "\n",
    "encoder_decoder_model.compile(loss='categorical_crossentropy', \n",
    "                       optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# check model stats\n",
    "\n",
    "stats = encoder_decoder_model.summary()\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/fit\n",
    "\n",
    "since we can generate as much data as we want whenever we want, there is no need for a train test split.\n",
    "\n",
    "unlimited data effectively means that we can show the model as many original samples as we want, without the model seeing a sample twice.\n",
    "\n",
    "in this way, the number of sequences generated is a proxy for the number of epochs. since we're just going to generate as many sequences as possible--rather than recylcing data--the number of epochs is 1.\n",
    "\n",
    "for this model, we'll use 100,000 training samples, with a batch size of 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100000/100000 [==============================] - 17s 169us/step - loss: 0.0820 - acc: 0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f12d3357400>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = generate_data(100000, n_terms, largest_number, alphabet)\n",
    "\n",
    "encoder_decoder_model.fit(X, y, epochs=1, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
