{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addition prediction problem & encoder-decoder lstm\n",
    "\n",
    "in their 2015 [paper](https://arxiv.org/pdf/1410.4615.pdf \"https://arxiv.org/pdf/1410.4615.pdf\"), Wojciech Zaremba and Ilya Sutskever showed that LSTM encoder-decoder models were capable of calculating the output of small programs--adding together two numbers of up to nine digits in length.\n",
    "\n",
    "even more impressive, the model was reading the character representations of the symbols and digits. there is nothing programatically to indicate to the model what operations are being represented--it learns this during training:\n",
    "\n",
    ">*It is important to emphasize that the LSTM reads the entire input one character at a time and produces the output one character at a time. The characters are initially meaningless from the model’s\n",
    "perspective;  for instance, the model does not know that “+” means addition or that\n",
    "6\n",
    "is followed\n",
    "by\n",
    "7\n",
    ".  In fact, scrambling the input characters (e.g., replacing “a” with “q”, “b” with “w”, etc.,) has\n",
    "no effect on the model’s ability to solve this problem*\n",
    ">\n",
    "> Wojciech Zaremba and Ilya Sutskever, Learning to Execute\n",
    "\n",
    "as an example of how this works, the model might take in the sequence representing\n",
    "\n",
    "12 + 4 = 16\n",
    "\n",
    "represented in the following vectors:\n",
    "\n",
    "`['1','2','+','5']`\n",
    "\n",
    "`['1','7']`\n",
    "\n",
    "the digits and symbols are just characters; they have no fucntional meaning. the model learns the relationships during training.\n",
    "\n",
    "\n",
    "## sequence-to-sequence (seq2seq) model\n",
    "\n",
    "there are a couple of key characteristics to note in this problem. first, the order matters--shuffling the order of the characters would make any relationship impossible to deduce.\n",
    "\n",
    "second, the input and output can vary, making this problem more challenging than a one-to-one or many-to-one sequence prediction problem.\n",
    "\n",
    "because the data is an ordered sequence of variable input and output length, this problem requires a many-to-many modeling approach, otherwise known as __sequence to sequence__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
