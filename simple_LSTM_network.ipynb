{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple LSTM\n",
    "\n",
    "so basic it runs on pumpkin spice\n",
    "\n",
    "## generating data\n",
    "\n",
    "### echo sequence prediction problem\n",
    "\n",
    "our echo sequence prediction problem needs data: specifically vectors of random sequences. let's use integers, and define our problem space as integers between 0 and 99.\n",
    "\n",
    "we'll use the ```randint()``` function from the python 3 ```random``` [module](https://docs.python.org/3/library/random.html \"python 3 random module docs\") to generate random integers within the range we specify (in this case, 0 to 99). \n",
    "\n",
    "we can use the ```randint()``` function within a function of our own to generate sequences of random integers--this will be the data for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randint() is inside the python random module\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use randint() to generate a random integer between 0 and 99\n",
    "\n",
    "rand_int = random.randint(0, 99)\n",
    "\n",
    "rand_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need a _lot_ more than one of these. which means it's time to build a function to automate this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seq(seq_length, n_features):\n",
    "    \n",
    "    '''\n",
    "    generate sequences of a given length\n",
    "    and given number of features\n",
    "    '''\n",
    "    return [random.randint(0, n_features - 1) for _ in range(seq_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__demo:__ let's make a sequence with 10 values and 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 25, 12, 40, 11, 36, 25, 42, 6, 25]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_seq(10, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one hot encoding\n",
    "\n",
    "before we can train the model, we have to encode the data into a format that an LSTM can use. the way we encode data matters; choices made here can significantly affect model performance.\n",
    "\n",
    "to frame this data properly, let's revisit the original problem:\n",
    "\n",
    "we're trying to predict a number. a _specific_ number.\n",
    "\n",
    "if we wanted to _approximate_ the number, we could frame this as a __regression__ problem, and train our model to output a close (but not exact) approximation of the number.\n",
    "\n",
    "but because we want the _exact_ integer (and _not_ an approximation, which is what a regression model outputs) we need to frame this problem as a __classification__ model.\n",
    "\n",
    "__classification__ means handling categorical data, which machines can do handily using __one hot encoding__.\n",
    "\n",
    "### automatic vs manual one hot encoding\n",
    "\n",
    "```scikit-learn``` has a super neat ```OneHotEncoder()``` [transformer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html \"sklearn OneHotEncoder doc\") that can automate one hot encoding, but because it fits the data, it can only encode the values that it sees represented. \n",
    "\n",
    "we need all possible values--from 0 to 99--represented. but because we're generating our integer sequences pseudo-randomly using ```np.random.randint()```, we can't guarantee that all values will be represented.\n",
    "\n",
    "it's possible to feed in the categories to ```OneHotEncoder()``` manually. here, however, we're going to simply make our own transformer.\n",
    "\n",
    "we'll convert the results to a ```numpy``` ```array``` in order to make them easier to decode later.\n",
    "\n",
    "### decoding\n",
    "\n",
    "later on we'll need a way to interpret the model's results. to do so we'll need to decode the one hot scheme.\n",
    "\n",
    "we can easily do this using the ```numpy``` ```argmax()``` function.\n",
    "\n",
    "```numpy.argmax()``` returns the indices for the maximum values along a vector. because each vector in the binary one hot encoding will be a lot of zeroes with a single high value--a ```1```--we can easily use ```argmax()``` to grab the index of the ```1``` value and return it. that's our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "# encoder function\n",
    "\n",
    "def one_hot_encoder(seq, n_features):\n",
    "    \n",
    "    '''\n",
    "    creates a vector of binary values for each\n",
    "    possible feature in the dataset.\n",
    "    '''\n",
    "    \n",
    "    encoding = list()\n",
    "    \n",
    "    for val in seq:\n",
    "        \n",
    "        vector = [0 for _ in range(n_features)]\n",
    "        vector[val] = 1\n",
    "        encoding.append(vector)\n",
    "        \n",
    "    return array(encoding)  \n",
    "\n",
    "# decoder function\n",
    "\n",
    "def one_hot_decoder(seq_encoded):\n",
    "    '''\n",
    "    decodes results by returning the index of\n",
    "    the point in the vector with the largest value,\n",
    "    i.e. 1 \n",
    "    '''\n",
    "    \n",
    "    return [argmax(vector) for vector in seq_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 0, 73, 47, 37, 53, 96, 51, 80, 54, 97, 91, 11, 27, 22, 18, 96, 50, 12, 7, 10, 18, 58, 1, 88, 16, 0, 52, 32, 46, 90, 39, 59, 96, 13, 50, 30, 30, 84, 7, 49, 88, 46, 90, 5, 9, 47, 31, 93, 78] \n",
      "\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [1 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] \n",
      "\n",
      "[37, 0, 73, 47, 37, 53, 96, 51, 80, 54, 97, 91, 11, 27, 22, 18, 96, 50, 12, 7, 10, 18, 58, 1, 88, 16, 0, 52, 32, 46, 90, 39, 59, 96, 13, 50, 30, 30, 84, 7, 49, 88, 46, 90, 5, 9, 47, 31, 93, 78]\n"
     ]
    }
   ],
   "source": [
    "seq = make_seq(50, 100)\n",
    "\n",
    "seq_encoded = one_hot_encoder(seq, 100)\n",
    "\n",
    "seq_decoded = one_hot_decoder(seq_encoded)\n",
    "\n",
    "print(seq, '\\n')\n",
    "print(seq_encoded, '\\n')\n",
    "print(seq_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshape to 3d matrix\n",
    "\n",
    "LSTMs require input in the form of a 3d matrix.\n",
    "\n",
    "the three dimensions LSTMs need, in order, are: __samples, time steps, & features__.\n",
    "\n",
    "the sequence we generated above, ```seq```, is \n",
    "\n",
    "* one __sample__,\n",
    "* fifty __time steps__, \n",
    "* one hundred __features__.\n",
    "\n",
    "for ```seq```, the specific sequence we just generated, it's easy to set the shape to three dimensions using the ```reshape()``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ..., 0 0 0]\n",
      "  [1 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]]\n",
      "(1, 50, 100)\n"
     ]
    }
   ],
   "source": [
    "X = seq_encoded.reshape(1, 50, 100)\n",
    "\n",
    "print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a more generalizable version might look like this:\n",
    "\n",
    "```X = seq_encoded.reshape(n_samples, length, n_features)```\n",
    "\n",
    "### generating samples\n",
    "\n",
    "following each of the steps above, in order, will generate 1 sample for our LSTM model.\n",
    "\n",
    "it makes sense to automate these tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(length, n_features, output_index):\n",
    "    '''\n",
    "    creates a single sample that is LSTM-ready.\n",
    "    '''\n",
    "    #create sequence of pseudo-random integers\n",
    "    seq = make_seq(length, n_features)\n",
    "    \n",
    "    # one hot encoding\n",
    "    seq_encoded = one_hot_encoder(seq, n_features)\n",
    "    \n",
    "    # reshape to 3d matrix suitable for LSTM\n",
    "    X = seq_encoded.reshape(1, length, n_features)\n",
    "    \n",
    "    # get the output\n",
    "    y = seq_encoded[output_index].reshape(1, n_features)\n",
    "    \n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's test ```make_sample``` to make sure it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  ..., \n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]\n",
      "  [0 0 0 ..., 0 0 0]]] \n",
      "\n",
      "(1, 50, 100) \n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] \n",
      "\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "X, y = make_sample(50, 100, 17)\n",
    "\n",
    "print(X, '\\n')\n",
    "print(X.shape, '\\n')\n",
    "print(y, '\\n')\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the model\n",
    "\n",
    "we'll build the model in 3 steps:\n",
    "\n",
    "1) __define__ and __compile__ the model\n",
    "\n",
    "2) __fit__ the model\n",
    "\n",
    "3) __evaluate__ the model\n",
    "\n",
    "define the model using the ```keras``` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the feature number and length in advance for cleaner code\n",
    "\n",
    "length = 10\n",
    "\n",
    "n_features = 25\n",
    "\n",
    "output_index = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create & define the model\n",
    "\n",
    "this simple model will consist of two layers: an __LSTM__ layer with 25 memory units, and a fully connected __dense__ layer with one neuron per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a Sequential() model\n",
    "\n",
    "lstm_model = Sequential()\n",
    "\n",
    "# add LSTM layer\n",
    "# 25 memory units\n",
    "\n",
    "lstm_model.add(LSTM(25, input_shape=(length, n_features)))\n",
    "\n",
    "# add dense layer\n",
    "# fully connected = 1 neuron for each feature\n",
    "# softmax for classification output\n",
    "\n",
    "lstm_model.add(Dense(n_features, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compile the model\n",
    "\n",
    "this is where we set parameters like the model's _loss function_, _optimizer_, and the specific performance-related information we want the machine to output as it trains.\n",
    "\n",
    "this example uses the [log loss function](http://wiki.fast.ai/index.php/Log_Loss \"log loss function\"), specified under the ```loss``` parameter as ```categorical_crossentropy```.\n",
    "\n",
    "this model uses the __adam__ optimizer, and outputs accuracy measurements ```acc``` each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                   metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit the model\n",
    "\n",
    "using the ```make_sample``` function we created above we could:\n",
    "\n",
    "1) make a large number of samples\n",
    "\n",
    "2) put them all together\n",
    "\n",
    "3) feed them into the model\n",
    "\n",
    "__however__, let's not do that here. the purpose of this notebook is to explore basic configuration of LSTMs using the ```keras``` api. \n",
    "\n",
    "for this model, we'll set one epoch to one sample, and clear the internal state in between epochs. this means our batch size = 1.\n",
    "\n",
    "this means that the model will train and test on one sample at a time.\n",
    "\n",
    "setting the ```verbose``` parameter equal to 2 will output training times, loss, and accuracy (either 1 or 0, for 0-100%) for each epoch. \n",
    "\n",
    "accuracy will be either 1 or 0 because we're training and testing on one sample at a time.\n",
    "\n",
    "#### manually fit the model\n",
    "\n",
    "we will train the model over ten thousand samples/epochs, with a batch size of 1 sample.\n",
    "\n",
    "it's easy to create a loop that will run the model on a new sample each time, using the ```make_sample``` function from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "\n",
    "for epoch in range(10000):\n",
    "    \n",
    "    X, y = make_sample(length, n_features, output_index)\n",
    "    \n",
    "    lstm_model.fit(X, y, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate the model\n",
    "\n",
    "because this is such a simple model/example, we won't do any fancy model evaluation or tuning here. \n",
    "\n",
    "a fast & easy way to check our model's predictions on new data is to run ```predict()``` a number of times, and see what percentage are correct. \n",
    "\n",
    "#### the importance of evaluating on unseen data\n",
    "\n",
    "since we are generating our own data, one batch at a time, the problem of testing on training data doesn't apply.\n",
    "\n",
    "__however__, it's important to note that if we were using an intact dataset, we would *definitely* want to keep some of the data aside to test the model on after it's fit. \n",
    "\n",
    "testing the model on data that it's already seen is pointless--it doesn't give any insight into the model's ability to generalize to unseen data. \n",
    "\n",
    "for datasets that already exist (and aren't being randomly generated as we go), keeping out a portion to test on in the begining is crucial. it's that special, reserved testing data that we would use to test our model now.\n",
    "\n",
    "because all this data is brand new, we aren't working with a __train test split__ this time.\n",
    "\n",
    "#### build a function\n",
    "\n",
    "as with anything i may want to do more than once, i'll go ahead and make a function.\n",
    "\n",
    "to implement this we'll use a simple counter and some arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    '''\n",
    "    evaluate model accuracy over\n",
    "    100 independently generated samples\n",
    "    '''\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        # counter\n",
    "        correct_preds = 0\n",
    "        \n",
    "        # create sample\n",
    "        X, y = make_sample(length, n_features, output_index)\n",
    "        \n",
    "        # generate predictions\n",
    "        y_hat = model.predict(X)\n",
    "        \n",
    "        # evaluate results\n",
    "        if one_hot_decoder(y_hat) == one_hot_decoder(y):\n",
    "            \n",
    "            # update counter\n",
    "            correct_preds += 1\n",
    "    \n",
    "    # arithmetic to get percentage of correct predictions\n",
    "    accuracy = correct_preds / 100 * 100.0\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's test the function (and our model) out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM model accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "lstm_accuracy = eval_model(lstm_model)\n",
    "\n",
    "print('LSTM model accuracy = %f' % lstm_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making predictions\n",
    "\n",
    "our lstm model is ready to make predictions!\n",
    "\n",
    "since all our data is generated new, this step (in this case) is almost identical to testing our model above. \n",
    "\n",
    "since this would be a more user-facing application, it might be nice to create a function that not only generates predictions on whatever new data it gets, but also outputs more verbose information to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X_, y_):\n",
    "    \n",
    "    y_hat = model.predict(X)\n",
    "    \n",
    "    # get original sequence\n",
    "    seq = [one_hot_decoder(x) for x in X_]\n",
    "    \n",
    "    # correct\n",
    "    correct = one_hot_decoder(y_)\n",
    "    \n",
    "    # model's prediction\n",
    "    predicted = one_hot_decoder(y_hat)\n",
    "    \n",
    "    return seq, predicted, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get predictions on a fresh sample\n",
    "\n",
    "time to test our model & helper functions we've created to see whether they can easily make useful predictions for a user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[23, 20, 24, 20, 6, 20, 16, 4, 6, 12]], [4], [4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_sample(length, n_features, output_index)\n",
    "\n",
    "get_predictions(lstm_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it appears our ```lstm_model``` is working! using the ```get_predictions``` function we can run this more than once, and confirm that the model is working well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[8, 0, 19, 15, 19, 0, 22, 20, 17, 21]], [20], [20])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate new data for each run\n",
    "# this could optionally be part of the get_predictions() function\n",
    "\n",
    "X, y = make_sample(length, n_features, output_index)\n",
    "\n",
    "get_predictions(lstm_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our model is performing optimally! we asked for the seventh number in a sequence, and running ```get_predictions``` repeatedly demonstrates our model is ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more information\n",
    "\n",
    "##### python 3 random module documentation:\n",
    "\n",
    "https://docs.python.org/3/library/random.html\n",
    "\n",
    "##### sklearn preprocessing documentation:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "##### a very cool blog post about how LSTMs work:\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
